<p align="center">
  <img src="https://img.shields.io/badge/GDPVal-Real%20Work%20Benchmark-blueviolet?style=for-the-badge" alt="GDPVal RealWorks" />
</p>

<h1 align="center">GDPVal RealWorks</h1>

<p align="center">
  <strong>Benchmark LLMs on real expert work â€” not academic toy problems.</strong><br/>
  <em>A YAML-driven experiment pipeline + live dashboard for the <a href="https://arxiv.org/abs/2510.04374">GDPVal</a> Gold Subset (220 tasks).</em>
</p>

<p align="center">
  <a href="https://github.com/hyeonsangjeon/gdpval-realworks/actions/workflows/deploy.yml">
    <img src="https://github.com/hyeonsangjeon/gdpval-realworks/actions/workflows/deploy.yml/badge.svg" alt="Deploy" />
  </a>
  <a href="https://github.com/hyeonsangjeon/gdpval-realworks/actions/workflows/batch-run.yml">
    <img src="https://github.com/hyeonsangjeon/gdpval-realworks/actions/workflows/batch-run.yml/badge.svg" alt="Batch Run" />
  </a>
  <a href="LICENSE">
    <img src="https://img.shields.io/badge/license-MIT-green.svg" alt="License" />
  </a>
</p>

<p align="center">
  <a href="https://hyeonsangjeon.github.io/gdpval-realworks/">ğŸŒ Live Dashboard</a> Â· 
  <a href="README_KR.md">ğŸ‡°ğŸ‡· í•œêµ­ì–´</a> Â· 
  <a href="batch-runner/README.md">ğŸ“– Batch Runner Docs</a> Â· 
  <a href="https://arxiv.org/abs/2510.04374">ğŸ“„ Paper</a>
</p>

---

## The Problem

Most LLM benchmarks test **academic reasoning** â€” math, code puzzles, trivia.  
None of that tells you whether a model can actually **do your job**.

**GDPVal** (GDP-level Validation) is different: **220 real-world expert tasks** across 11 sectors and 55 occupations â€” Excel reports, legal docs, sales decks, the stuff people actually get paid for.

This repo automates the entire loop: **configure â†’ run â†’ collect â†’ visualize** â€” driven by a single YAML file, executed on GitHub Actions, results on a live dashboard.

> ğŸ¯ One YAML file. One button click. Full experiment lifecycle.

---

## How It Works

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                        YAML Experiment Config                       â”‚
  â”‚   model, prompts, filters, Self-QA, execution mode â€” all in one    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚              GitHub Actions  (workflow_dispatch)                     â”‚
  â”‚                                                                      â”‚
  â”‚  Step 0  Bootstrap      Clone openai/gdpval â†’ your HF repo          â”‚
  â”‚  Step 1  Prepare        Load dataset, apply filters                  â”‚
  â”‚  Step 2  Inference      Call LLM (Azure/OpenAI/Anthropic)            â”‚
  â”‚  Step 3  Format         JSON + Markdown report                       â”‚
  â”‚  Step 4  Fill Parquet   Merge results into submission format          â”‚
  â”‚  Step 5  Validate       Pre-upload integrity checks                  â”‚
  â”‚  Step 6  Upload         Push to HuggingFace Hub                      â”‚
  â”‚                                                                      â”‚
  â”‚  â†’ Auto-creates PR with experiment summary                           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                     React Dashboard (GitHub Pages)                   â”‚
  â”‚          Experiment results Â· Grade details Â· Cross-model comparison â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ Quick Start

### 1. Fork & Clone

```bash
git clone https://github.com/hyeonsangjeon/gdpval-realworks.git
cd gdpval-realworks
```

### 2. Configure GitHub Repository Settings

#### ğŸ”‘ Secrets

Go to **Settings â†’ Secrets and variables â†’ Actions â†’ New repository secret** and add the secrets you need:

| Secret Name | Value | Required? |
|---|---|---|
| `AZURE_OPENAI_API_KEY` | Azure OpenAI API key | âœ… If using Azure |
| `AZURE_OPENAI_ENDPOINT` | `https://your-resource.openai.azure.com/` | âœ… If using Azure |
| `OPENAI_API_KEY` | OpenAI API key | If using OpenAI |
| `ANTHROPIC_API_KEY` | Anthropic API key | If using Anthropic |
| `HF_TOKEN` | HuggingFace write token ([get one here](https://huggingface.co/settings/tokens)) | âœ… For upload |

> ğŸ’¡ You don't need all of them â€” just the provider you'll actually use.  
> For Azure users: `AZURE_OPENAI_API_KEY` + `AZURE_OPENAI_ENDPOINT` + `HF_TOKEN` is the minimum.

#### ğŸ“„ GitHub Pages

**Settings â†’ Pages â†’ Source** â†’ change to **"GitHub Actions"** (not "Deploy from a branch")

#### ğŸ”“ Workflow Permissions

**Settings â†’ Actions â†’ General â†’ Workflow permissions:**

- âœ… Select **"Read and write permissions"**
- âœ… Check **"Allow GitHub Actions to create and approve pull requests"**
- Save

#### ğŸ§¹ Auto-cleanup (recommended)

**Settings â†’ General** â†’ âœ… Check **"Automatically delete head branches"**

> This cleans up experiment branches automatically after PR merge.

---

### 3. Run Your First Experiment

1. Go to **Actions** tab â†’ **"Run GDPVal Batch Experiment"**
2. Click **"Run workflow"**
3. Fill in:
   - `experiment_yaml`: `exp998_smoke_baseline_sample` (smoke test, 3 tasks)
   - `dry_run`: âœ… checked (first time â€” skip upload)
4. Click **Run workflow** ğŸš€

```
âœ… Step 0: Bootstrap        â†’ HF repo ready
âœ… Step 1: Prepare tasks    â†’ 3 tasks filtered
âœ… Step 2: Run inference    â†’ LLM called for each task
âœ… Step 3: Format results   â†’ JSON + Markdown generated
âœ… Step 4: Fill parquet     â†’ Submission parquet ready
â­ï¸ Step 5: Validate        â†’ Skipped (smoke test)
â­ï¸ Step 6: Upload          â†’ Skipped (dry run)
```

> ğŸ‰ If this passes, uncheck `dry_run` and run a full experiment!

---

## ğŸ“ Write Your Own Experiment

Create a YAML file in `batch-runner/experiments/`:

```yaml
experiment:
  id: "exp001_GPT52Chat_baseline"
  name: "GPT-5.2 Chat Baseline (Full 220 tasks)"
  description: "Full baseline run with code_interpreter and Self-QA."

data:
  source: "HyeonSang/exp001_GPT52Chat_baseline"
  filter:
    sector: null          # null = all sectors
    sample_size: null     # null = all 220 tasks

condition_a:
  name: "Baseline"
  model:
    provider: "azure"
    deployment: "gpt-5.2-chat"
    temperature: 0.0
    seed: 42
  prompt:
    system: "You are a helpful assistant that completes professional tasks."
    suffix: "Generate actual files, not descriptions."
  qa:
    enabled: true
    min_score: 6
    max_retries: 3

# condition_b:            â† Add for A/B comparison (optional)

execution:
  mode: "code_interpreter"
  max_retries: 5
  resume_max_rounds: 3
```

Then trigger it from **Actions â†’ Run workflow** with `experiment_yaml: exp001_GPT52Chat_baseline`.

---

## ğŸ§  Execution Modes

| Mode | How It Works | Best For |
|---|---|---|
| **`code_interpreter`** | LLM writes + runs code inside Azure/OpenAI's **secure sandbox**. Files generated in the cloud. | âœ… Production â€” safe, powerful |
| **`subprocess`** | LLM generates code â†’ executed locally in an isolated temp directory. | Non-OpenAI models (Anthropic, etc.) |
| **`json_renderer`** | LLM outputs a JSON spec â†’ a **fixed renderer** creates files. Same renderer for all models. | Fair A/B comparison across models |

> ğŸ³ `subprocess` mode is planned to evolve into a **container-based** execution mode â€” if time permits and coffee supply holds.

---

## ğŸ”¬ Self-QA: Built-in Quality Gate

Every task output is inspected by the LLM itself before acceptance:

```
Task â†’ LLM generates output â†’ Self-QA inspects â†’ Score â‰¥ 6? 
                                                    â”œâ”€â”€ âœ… Accept
                                                    â””â”€â”€ âŒ Retry (up to 3Ã—)
```

Self-QA checks: Are all requirements met? Are files actually produced? Is the output professional?

---

## ğŸ—ï¸ Architecture

```
gdpval-realworks/
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ batch-run.yml          # ğŸ”¬ Experiment pipeline (workflow_dispatch)
â”‚   â””â”€â”€ deploy.yml             # ğŸŒ Dashboard deploy (push to main)
â”‚
â”œâ”€â”€ batch-runner/              # ğŸ Python pipeline
â”‚   â”œâ”€â”€ step0~step6            # Modular shell + Python scripts
â”‚   â”œâ”€â”€ core/                  # Business logic (LLM client, executor, etc.)
â”‚   â”œâ”€â”€ experiments/           # YAML experiment configs
â”‚   â”œâ”€â”€ prompts/               # Prompt templates
â”‚   â””â”€â”€ tests/                 # pytest suite
â”‚
â”œâ”€â”€ src/                       # âš›ï¸ React + Vite dashboard
â”‚   â”œâ”€â”€ pages/                 # Dashboard, ExperimentDetail, GradeDetail
â”‚   â””â”€â”€ components/            # UI components (shadcn/ui + Tailwind)
â”‚
â”œâ”€â”€ data/                      # ğŸ“Š Experiment results (JSON/YAML)
â”‚   â”œâ”€â”€ tests/                 # Experiment test data
â”‚   â””â”€â”€ grades/                # Grading results
â”‚
â””â”€â”€ scripts/                   # ğŸ”§ Build-time aggregation
    â”œâ”€â”€ aggregate-tests.mjs    # Collect experiment data â†’ JSON index
    â””â”€â”€ aggregate-grades.mjs   # Collect grade data â†’ JSON index
```

---

## ğŸ”„ GitHub Actions Workflows

### `batch-run.yml` â€” Run Experiments

| Feature | Detail |
|---|---|
| **Trigger** | Manual (`workflow_dispatch`) from Actions tab |
| **Input** | Experiment YAML filename + optional dry_run flag |
| **Pipeline** | Step 0 â†’ Step 6 (bootstrap â†’ upload) |
| **Smart skips** | Smoke tests skip validation; dry_run skips upload + PR |
| **Auto PR** | Creates a Pull Request with experiment summary |
| **Artifacts** | Full workspace uploaded for 30 days |
| **Timeout** | 5 hours max |

### `deploy.yml` â€” Deploy Dashboard

| Feature | Detail |
|---|---|
| **Trigger** | Push to `main` (auto) or manual |
| **Build** | Aggregate test/grade data â†’ React build â†’ GitHub Pages |
| **Scope** | Only runs when `data/`, `src/`, or `scripts/` change |

---

## ğŸ–¥ï¸ Dashboard

> â³ **Grading results are currently in progress.** The dashboard will be populated once [evals.openai.com](https://evals.openai.com/) finishes scoring â€” this can take a while.

The React dashboard at [hyeonsangjeon.github.io/gdpval-realworks](https://hyeonsangjeon.github.io/gdpval-realworks/) shows:

- ğŸ“Š **Experiment overview** â€” all runs with status and metadata
- ğŸ“ˆ **Grade summaries** â€” per-model scores across all tasks
- ğŸ” **Detailed views** â€” drill into individual task results
- âš–ï¸ **Comparison** â€” side-by-side model performance

Data is aggregated at build time from `data/` â†’ static JSON â†’ served by Vite.

---

## ğŸ§ª Testing

```bash
cd batch-runner
pip install -r requirements.txt

# Unit tests only (no API keys needed)
pytest

# Integration tests (requires real credentials)
pytest -m integration

# With coverage
pytest --cov=core --cov-report=html
```

### ğŸ–¥ï¸ Run Locally (step by step)

```bash
cd batch-runner
export HF_TOKEN="hf_xxx"
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com"
export AZURE_OPENAI_API_KEY="xxx"

./step0_bootstrap.sh experiments/exp998_smoke_baseline_sample.yaml
./step1_prepare_tasks.sh experiments/exp998_smoke_baseline_sample.yaml
./step2_run_inference.sh condition_a
./step3_format_results.sh
./step4_fill_parquet.sh
./step5_validate.sh
./step6_upload_hf.sh --test
```

> ğŸ’¡ Local execution works, but for full 220-task runs we recommend **GitHub Actions**.  
> The batch workflow parallelizes as fast as your TPM (Tokens Per Minute) quota allows â€” let the cloud do the heavy lifting while you grab a coffee. â˜•

---

## ğŸ“š References

- **GDPVal Paper**: [arXiv:2510.04374](https://arxiv.org/abs/2510.04374)
- **GDPVal Dataset**: [openai/gdpval](https://huggingface.co/datasets/openai/gdpval)
- **GDPVal Grading**: [evals.openai.com](https://evals.openai.com/)
- **Azure OpenAI Responses API**: [Documentation](https://learn.microsoft.com/azure/ai-services/openai/)

---

## ğŸ‘¤ Author

**Hyeonsang Jeon**  
Sr. Solution Engineer Â· Global Black Belt â€” AI Apps | Microsoft Asia, Korea  
[![GitHub](https://img.shields.io/badge/GitHub-hyeonsangjeon-181717?logo=github)](https://github.com/hyeonsangjeon)
[![Dashboard](https://img.shields.io/badge/Live%20Dashboard-GDPVal-blueviolet?logo=react)](https://hyeonsangjeon.github.io/gdpval-realworks/experiments)

---

## ğŸ“„ License

MIT â€” See [LICENSE](LICENSE) for details.
