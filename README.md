<p align="center">
  <img src="https://img.shields.io/badge/GDPVal-Real%20Work%20Benchmark-blueviolet?style=for-the-badge" alt="GDPVal RealWorks" />
</p>

<h1 align="center">GDPVal RealWorks</h1>

<p align="center">
  <strong>Benchmark LLMs on real expert work â€” not academic toy problems.</strong><br/>
  <em>A YAML-driven experiment pipeline + live dashboard for the <a href="https://arxiv.org/abs/2510.04374">GDPVal</a> Gold Subset (220 tasks).</em>
</p>

<p align="center">
  <a href="https://github.com/hyeonsangjeon/gdpval-realworks/actions/workflows/deploy.yml">
    <img src="https://github.com/hyeonsangjeon/gdpval-realworks/actions/workflows/deploy.yml/badge.svg" alt="Deploy" />
  </a>
  <a href="https://github.com/hyeonsangjeon/gdpval-realworks/actions/workflows/batch-run.yml">
    <img src="https://github.com/hyeonsangjeon/gdpval-realworks/actions/workflows/batch-run.yml/badge.svg" alt="Batch Run" />
  </a>
  <a href="LICENSE">
    <img src="https://img.shields.io/badge/license-MIT-green.svg" alt="License" />
  </a>
</p>

<p align="center">
  <a href="https://hyeonsangjeon.github.io/gdpval-realworks/">ğŸŒ Live Dashboard</a> Â· 
  <a href="README_KR.md">ğŸ‡°ğŸ‡· í•œêµ­ì–´</a> Â· 
  <a href="batch-runner/README.md">ğŸ“– Batch Runner Docs</a> Â· 
  <a href="https://arxiv.org/abs/2510.04374">ğŸ“„ Paper</a>
</p>

---

## The Problem

Most LLM benchmarks test **academic reasoning** â€” math, code puzzles, trivia.  
None of that tells you whether a model can actually **do your job**.

**GDPVal** (GDP-level Validation) is different: **220 real-world expert tasks** across 11 sectors and 55 occupations â€” Excel reports, legal docs, sales decks, the stuff people actually get paid for.

This repo automates the entire loop: **configure â†’ run â†’ collect â†’ visualize** â€” driven by a single YAML file, executed on GitHub Actions, results on a live dashboard.

> ğŸ¯ One YAML file. One button click. Full experiment lifecycle.

---

## How It Works

<table>
<tr>
<td align="center"><img src="https://mermaid.ink/img/Zmxvd2NoYXJ0IFRECiAgICBBWyJTdGVwIDA6IEJvb3RzdHJhcDxicj5IRiByZXBvICsgc25hcHNob3QiXSAtLT4gQlsiU3RlcCAxOiBQcmVwYXJlPGJyPkZpbHRlciArIGxvYWQgdGFza3MiXQ==" alt="Preparation" width="350" /></td>
<td align="center" style="font-size:2em;">â†’</td>
<td align="center"><img src="https://mermaid.ink/img/Zmxvd2NoYXJ0IFRECiAgICBDWyJTdGVwIDI6IEluZmVyZW5jZTxicj5MTE0gKyBTZWxmLVFBIl0gLS0-IERbIlN0ZXAgMzogRm9ybWF0PGJyPkpTT04gKyBNYXJrZG93biJd" alt="Execution" width="350" /></td>
</tr>
<tr>
<td></td>
<td align="center" style="font-size:2em;">â†“</td>
<td></td>
</tr>
<tr>
<td align="center"><img src="https://mermaid.ink/img/Zmxvd2NoYXJ0IFRECiAgICBFWyJTdGVwIDQ6IFBhcnF1ZXQ8YnI-TWVyZ2Ugc3VibWlzc2lvbiJdIC0tPiBGWyJTdGVwIDU6IFZhbGlkYXRlPGJyPkludGVncml0eSBjaGVjayJd" alt="Delivery" width="350" /></td>
<td align="center" style="font-size:2em;">â†’</td>
<td align="center"><img src="https://mermaid.ink/img/Zmxvd2NoYXJ0IFRECiAgICBHWyJTdGVwIDY6IFJlcG9ydDxicj5IVE1MICsgSlNPTiJdIC0tPiBIWyJTdGVwIDc6IFVwbG9hZDxicj5IRiArIEF1dG8gUFIiXQ==" alt="Report & Upload" width="350" /></td>
</tr>
</table>


---

## âš¡ Quick Start

### 1. Fork & Clone

```bash
git clone https://github.com/hyeonsangjeon/gdpval-realworks.git
cd gdpval-realworks
```

### 2. Configure GitHub Repository Settings

#### ğŸ”‘ Secrets

Go to **Settings â†’ Secrets and variables â†’ Actions â†’ New repository secret** and add the secrets you need:

| Secret Name | Value | Required? |
|---|---|---|
| `AZURE_OPENAI_API_KEY` | Azure OpenAI API key | âœ… If using Azure |
| `AZURE_OPENAI_ENDPOINT` | `https://your-resource.openai.azure.com/` | âœ… If using Azure |
| `OPENAI_API_KEY` | OpenAI API key | If using OpenAI |
| `ANTHROPIC_API_KEY` | Anthropic API key | If using Anthropic |
| `HF_TOKEN` | HuggingFace write token ([get one here](https://huggingface.co/settings/tokens)) | âœ… For upload |

> ğŸ’¡ You don't need all of them â€” just the provider you'll actually use.  
> For Azure users: `AZURE_OPENAI_API_KEY` + `AZURE_OPENAI_ENDPOINT` + `HF_TOKEN` is the minimum.

#### ğŸ“„ GitHub Pages

**Settings â†’ Pages â†’ Source** â†’ change to **"GitHub Actions"** (not "Deploy from a branch")

#### ğŸ”“ Workflow Permissions

**Settings â†’ Actions â†’ General â†’ Workflow permissions:**

- âœ… Select **"Read and write permissions"**
- âœ… Check **"Allow GitHub Actions to create and approve pull requests"**
- Save

#### ğŸ§¹ Auto-cleanup (recommended)

**Settings â†’ General** â†’ âœ… Check **"Automatically delete head branches"**

> This cleans up experiment branches automatically after PR merge.

---

### 3. Run Your First Experiment

1. Go to **Actions** tab â†’ **"Run GDPVal Batch Experiment"**
2. Click **"Run workflow"**
3. Fill in:
   - `experiment_yaml`: `exp998_smoke_baseline_sample` (smoke test, 3 tasks)
   - `dry_run`: âœ… checked (first time â€” skip upload)
4. Click **Run workflow** ğŸš€

```
âœ… Step 0: Bootstrap        â†’ HF repo ready
âœ… Step 1: Prepare tasks    â†’ 3 tasks filtered
âœ… Step 2: Run inference    â†’ LLM called for each task
âœ… Step 3: Format results   â†’ JSON + Markdown generated
âœ… Step 4: Fill parquet     â†’ Submission parquet ready
â­ï¸ Step 5: Validate        â†’ Skipped (smoke test)
â­ï¸ Step 6: Upload          â†’ Skipped (dry run)
```

> ğŸ‰ If this passes, uncheck `dry_run` and run a full experiment!

---

## ğŸ“ Write Your Own Experiment

Create a YAML file in `batch-runner/experiments/`:

```yaml
experiment:
  id: "exp001_GPT52Chat_baseline"
  name: "GPT-5.2 Chat Baseline (Full 220 tasks)"
  description: "Full baseline run with code_interpreter and Self-QA."

data:
  source: "HyeonSang/exp001_GPT52Chat_baseline"
  filter:
    sector: null          # null = all sectors
    sample_size: null     # null = all 220 tasks

condition_a:
  name: "Baseline"
  model:
    provider: "azure"
    deployment: "gpt-5.2-chat"
    temperature: 0.0
    seed: 42
  prompt:
    system: "You are a helpful assistant that completes professional tasks."
    suffix: "Generate actual files, not descriptions."
  qa:
    enabled: true
    min_score: 6
    max_retries: 3

# condition_b:            â† Add for A/B comparison (optional)

execution:
  mode: "code_interpreter"
  max_retries: 5
  resume_max_rounds: 3
```

Then trigger it from **Actions â†’ Run workflow** with `experiment_yaml: exp001_GPT52Chat_baseline`.

---

## ğŸ§  Execution Modes

| Mode | How It Works | Best For |
|---|---|---|
| **`code_interpreter`** | LLM writes + runs code inside Azure/OpenAI's **secure sandbox**. Files generated in the cloud. | âœ… Production â€” safe, powerful |
| **`subprocess`** | LLM generates code â†’ executed locally in an isolated temp directory. | Non-OpenAI models (Anthropic, etc.) |
| **`json_renderer`** | LLM outputs a JSON spec â†’ a **fixed renderer** creates files. Same renderer for all models. | Fair A/B comparison across models |

> ğŸ³ `subprocess` mode is planned to evolve into a **container-based** execution mode â€” if time permits and coffee supply holds.

---

## ğŸ”¬ Self-QA: Built-in Quality Reflection Gate

Before acceptance, the same LLM working on the task inspects its own output:
Self-QA scores each output on a 0-10 scale using rubric-based self-evaluation. If the score is below the configured threshold (default: 6), it enters a reflection loop and retries.

<img src="https://mermaid.ink/img/Zmxvd2NoYXJ0IExSCiAgICB0YXNrWyJUYXNrIl0gLS0-IGdlblsiTExNIEdlbmVyYXRlcyBPdXRwdXQiXSAtLT4gcWFbIlNlbGYtUUEgSW5zcGVjdHMiXSAtLT4gZ2F0ZXsiU2NvcmUgPj0gNj8ifQogICAgZ2F0ZSAtLT58WWVzfCBhY2NlcHRbIkFjY2VwdCJdCiAgICBnYXRlIC0tPnxOb3wgcmV0cnlbIlJldHJ5ICh1cCB0byAzeCkiXQo=" alt="Self-QA Flow" />


Self-QA checks: Are all requirements met? Are files actually produced? Is the output professional?

---

## ğŸ—ï¸ Architecture

<img src="https://mermaid.ink/img/Zmxvd2NoYXJ0IFRCCiAgICByb290WyJnZHB2YWwtcmVhbHdvcmtzLyJdCgogICAgd2ZbIi5naXRodWIvd29ya2Zsb3dzLzxici8-YmF0Y2gtcnVuLnltbCwgZGVwbG95LnltbCJdCiAgICBiclsiYmF0Y2gtcnVubmVyLzxici8-c3RlcCBzY3JpcHRzLCBjb3JlLCBleHBlcmltZW50cywgcHJvbXB0cywgdGVzdHMiXQogICAgc3JjWyJzcmMvPGJyLz5wYWdlcywgY29tcG9uZW50cyJdCiAgICBkYXRhWyJkYXRhLzxici8-dGVzdHMsIGdyYWRlcyJdCiAgICBzY3JpcHRzWyJzY3JpcHRzLzxici8-YWdncmVnYXRlLXRlc3RzLm1qcywgYWdncmVnYXRlLWdyYWRlcy5tanMiXQoKICAgIHJvb3QgLS0-IHdmCiAgICByb290IC0tPiBicgogICAgcm9vdCAtLT4gc3JjCiAgICByb290IC0tPiBkYXRhCiAgICByb290IC0tPiBzY3JpcHRzCg==" alt="Architecture" />


---

## ğŸ”„ GitHub Actions Workflows

### `batch-run.yml` â€” Run Experiments

| Feature | Detail |
|---|---|
| **Trigger** | Manual (`workflow_dispatch`) from Actions tab |
| **Input** | Experiment YAML filename + optional dry_run flag |
| **Pipeline** | Step 0 â†’ Step 7 (bootstrap â†’ upload) |
| **Smart skips** | Smoke tests skip validation; dry_run skips upload + PR |
| **Auto PR** | Creates a Pull Request with experiment summary |
| **Artifacts** | Full workspace uploaded for 30 days |
| **Timeout** | 5 hours max |

### `deploy.yml` â€” Deploy Dashboard

| Feature | Detail |
|---|---|
| **Trigger** | Push to `main` (auto) or manual |
| **Build** | Aggregate test/grade data â†’ React build â†’ GitHub Pages |
| **Scope** | Only runs when `data/`, `src/`, or `scripts/` change |

---

## ğŸ–¥ï¸ Dashboard

> â³ **Grading results are currently in progress.** The dashboard will be populated once [evals.openai.com](https://evals.openai.com/) finishes scoring â€” this can take a while.

The React dashboard at [hyeonsangjeon.github.io/gdpval-realworks](https://hyeonsangjeon.github.io/gdpval-realworks/) shows:

- ğŸ“Š **Experiment overview** â€” all runs with status and metadata
- ğŸ“ˆ **Grade summaries** â€” per-model scores across all tasks
- ğŸ” **Detailed views** â€” drill into individual task results
- âš–ï¸ **Comparison** â€” side-by-side model performance

Data is aggregated at build time from `data/` â†’ static JSON â†’ served by Vite.

---

## ğŸ§ª Testing

```bash
cd batch-runner
pip install -r requirements.txt

# Unit tests only (no API keys needed)
pytest

# Integration tests (requires real credentials)
pytest -m integration

# With coverage
pytest --cov=core --cov-report=html
```

### ğŸ–¥ï¸ Run Locally (step by step)

```bash
cd batch-runner
export HF_TOKEN="hf_xxx"
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com"
export AZURE_OPENAI_API_KEY="xxx"

./step0_bootstrap.sh experiments/exp998_smoke_baseline_sample.yaml
./step1_prepare_tasks.sh experiments/exp998_smoke_baseline_sample.yaml
./step2_run_inference.sh condition_a
./step3_format_results.sh
./step4_fill_parquet.sh
./step5_validate.sh
./step6_report.sh
./step7_upload_hf.sh --test
```

> ğŸ’¡ Local execution works, but for full 220-task runs we recommend **GitHub Actions**.  
> The batch workflow parallelizes as fast as your TPM (Tokens Per Minute) quota allows â€” let the cloud do the heavy lifting while you grab a coffee. â˜•

---

## ğŸ“š References

- **GDPVal Paper**: [arXiv:2510.04374](https://arxiv.org/abs/2510.04374)
- **GDPVal Dataset**: [openai/gdpval](https://huggingface.co/datasets/openai/gdpval)
- **GDPVal Grading**: [evals.openai.com](https://evals.openai.com/)
- **Azure OpenAI Responses API**: [Documentation](https://learn.microsoft.com/azure/ai-services/openai/)

---

## ğŸ‘¤ Author

**Hyeonsang Jeon**  
Sr. Solution Engineer Â· Global Black Belt â€” AI Apps | Microsoft Asia, Korea  
[![GitHub](https://img.shields.io/badge/GitHub-hyeonsangjeon-181717?logo=github)](https://github.com/hyeonsangjeon)
[![Dashboard](https://img.shields.io/badge/Live%20Dashboard-GDPVal-blueviolet?logo=react)](https://hyeonsangjeon.github.io/gdpval-realworks/)

---

## ğŸ“„ License

MIT â€” See [LICENSE](LICENSE) for details.
