#!/usr/bin/env python3
"""Step 2: Run Inference â€” Call LLM for each task, save results incrementally.

Resume ë™ì‘:
  - step2_inference_progress.jsonì—ì„œ error/qa_failed íƒœìŠ¤í¬ë¥¼ ì°¾ì•„ ì¬ì‹¤í–‰
  - resume_max_roundsë§Œí¼ ë°˜ë³µ (YAML execution.resume_max_rounds)
  - ì„±ê³µí•œ íƒœìŠ¤í¬ëŠ” progress.jsonì—ì„œ ì§ì ‘ ì—…ë°ì´íŠ¸ (ì˜¤ë¸Œì íŠ¸ êµì²´)
  - ëª¨ë“  ë¼ìš´ë“œ ëë‚˜ë©´ ìµœì¢… ê²°ê³¼ë¥¼ step2_inference_results.jsonì— ì €ì¥

Input:
  - workspace/step1_tasks_prepared.json  (from Step 1)
  - workspace/step0_needs_files_manifest.json (from Step 0)

Output:
  - workspace/step2_inference_results.json   (final)
  - workspace/step2_inference_progress.json  (incremental, for resume)
  - workspace/upload/deliverable_files/<task_id>/  (generated files)

Usage:
    python step2_run_inference.py --condition condition_a
    python step2_run_inference.py --condition condition_a --no-resume
    python step2_run_inference.py --condition condition_a --mode subprocess  # CLI override
"""

import argparse
import json
import os
import re
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, List

from core.config import WORKSPACE_DIR, UPLOAD_DIR, DELIVERABLE_DIR, DEFAULT_LOCAL_PATH
from core.data_loader import GDPValTask
from core.executor import TaskExecutor
from core.file_preview import generate_all_previews
from core.llm_client import create_client, create_provider_client, complete
from core.needs_files import NeedsFilesManifest
from core.prompt_builder import PromptBuilder, PromptConfig as BuilderPromptConfig


# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

RETRIABLE_STATUSES = {"error", "qa_failed"}

# temperature=0 ë¯¸ì§€ì› ëª¨ë¸ ìºì‹œ (ëŸ°íƒ€ì„ì— í•™ìŠµ, ì„¸ì…˜ ë‚´ ì¬ì‚¬ìš©)
_MODELS_NO_TEMPERATURE: set = set()


# â”€â”€ JSON extraction helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _extract_json_from_response(raw: str) -> dict:
    """Extract JSON from LLM response with multiple fallback strategies.

    Tries:
      0. Strip <think>...</think> tags (reasoning models)
      1. Direct JSON parse
      2. ```json ... ``` code fence extraction
      3. First balanced { ... } block extraction
      4. Truncated JSON repair (close open strings/brackets/braces)
      5. Regex extraction of essential fields (score, passed)

    Raises:
        ValueError: if all strategies fail
    """
    if not raw or not raw.strip():
        raise ValueError("Empty response")

    text = raw.strip()

    # Strategy 0: Remove <think>...</think> tags (reasoning models like o1, gpt-5)
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()

    # Strategy 1: Direct parse
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    # Strategy 2: ```json code fence
    json_match = re.search(r"```(?:json)?\s*\n?(.*?)```", text, re.DOTALL)
    if json_match:
        candidate = json_match.group(1).strip()
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            repaired = _try_repair_truncated_json(candidate)
            if repaired is not None:
                return repaired

    # Strategy 3: First balanced { ... } block (brace-depth matching)
    depth = 0
    start = None
    for i, ch in enumerate(text):
        if ch == '{':
            if depth == 0:
                start = i
            depth += 1
        elif ch == '}':
            depth -= 1
            if depth == 0 and start is not None:
                try:
                    return json.loads(text[start:i + 1])
                except json.JSONDecodeError:
                    start = None  # try next { block

    # Strategy 4: Truncated JSON repair
    brace_start = text.find('{')
    if brace_start >= 0:
        repaired = _try_repair_truncated_json(text[brace_start:])
        if repaired is not None:
            return repaired

    # Strategy 5: Regex extraction of essential fields
    essential = _extract_essential_fields(text)
    if essential is not None:
        return essential

    raise ValueError(f"No JSON found in response: {text[:100]}")


def _try_repair_truncated_json(text: str) -> dict | None:
    """Attempt to repair a truncated JSON string.

    Common case: LLM QA response cut off mid-JSON due to max_tokens.
    Returns parsed dict on success, None on failure.
    """
    if not text or '{' not in text:
        return None

    # Already valid?
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    # Strategy A: Track state and truncate at last safe comma
    in_string = False
    escape = False
    last_comma = -1
    depth_brace = 0
    depth_bracket = 0

    for i, ch in enumerate(text):
        if escape:
            escape = False
            continue
        if ch == '\\' and in_string:
            escape = True
            continue
        if ch == '"':
            in_string = not in_string
            continue
        if in_string:
            continue
        if ch == '{':
            depth_brace += 1
        elif ch == '}':
            depth_brace -= 1
        elif ch == '[':
            depth_bracket += 1
        elif ch == ']':
            depth_bracket -= 1
        elif ch == ',':
            last_comma = i

    # If all brackets are balanced, try closing open strings
    if depth_brace == 0 and depth_bracket == 0:
        attempt = text
        if attempt.count('"') % 2 == 1:
            attempt += '"'
        try:
            return json.loads(attempt)
        except json.JSONDecodeError:
            pass

    # Truncate at last comma (removes incomplete trailing value)
    if last_comma > 0:
        truncated = text[:last_comma]
        # Recount open structures after truncation
        ob = truncated.count('[') - truncated.count(']')
        oc = truncated.count('{') - truncated.count('}')
        truncated += ']' * max(0, ob) + '}' * max(0, oc)
        try:
            return json.loads(truncated)
        except json.JSONDecodeError:
            pass

    # Strategy B: Brute-force close from end
    attempt = text
    if attempt.count('"') % 2 == 1:
        attempt += '"'
    attempt += ']' * max(0, attempt.count('[') - attempt.count(']'))
    attempt += '}' * max(0, attempt.count('{') - attempt.count('}'))
    try:
        result = json.loads(attempt)
        result.setdefault("passed", result.get("score", 0) >= 6)
        result.setdefault("score", 5)
        result.setdefault("issues", [])
        result.setdefault("suggestion", "")
        return result
    except json.JSONDecodeError:
        pass

    # Strategy C: Regex fallback
    return _extract_essential_fields(text)


def _extract_essential_fields(text: str) -> dict | None:
    """JSON íŒŒì‹±ì´ ì™„ì „íˆ ì‹¤íŒ¨í–ˆì„ ë•Œ regexë¡œ í•„ìˆ˜ í•„ë“œë§Œ ì¶”ì¶œ."""
    score_match = re.search(r'"score"\s*:\s*(\d+)', text)
    passed_match = re.search(r'"passed"\s*:\s*(true|false)', text, re.IGNORECASE)

    if score_match:
        score = int(score_match.group(1))
        if passed_match:
            passed = passed_match.group(1).lower() == "true"
        else:
            passed = score >= 6

        # issues ì¶”ì¶œ ì‹œë„
        issues = []
        issues_match = re.search(r'"issues"\s*:\s*\[(.*?)\]', text, re.DOTALL)
        if issues_match:
            issues = re.findall(r'"([^"]+)"', issues_match.group(1))

        return {
            "passed": passed,
            "score": score,
            "issues": issues,
            "suggestion": "",
        }

    return None


# â”€â”€ Self-QA: LLMì´ ìê¸° ê²°ê³¼ë¬¼ì„ ê²€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _run_self_qa(
    task_info: dict,
    condition: dict,
    deliverable_text: str,
    deliverable_files: list,
    client,
) -> dict:
    """
    LLMì´ QA ê²€ìˆ˜ê´€ ì—­í• ë¡œ ê²°ê³¼ë¬¼ í‰ê°€.

    Returns:
        {
            "passed": bool | None,  # None = undetermined (parse/API ì‹¤íŒ¨)
            "score": int | None,    # None = undetermined
            "issues": [...],
            "suggestion": str,
            "undetermined": bool,   # True = QA íŒì • ë¶ˆê°€
        }
    """
    qa_cfg = condition.get("qa", {})
    if not qa_cfg.get("enabled", False):
        return {"passed": True, "score": 10, "issues": [], "suggestion": "", "undetermined": False}

    qa_prompt_template = qa_cfg.get("prompt", "")
    if not qa_prompt_template:
        return {"passed": True, "score": 10, "issues": [], "suggestion": "", "undetermined": False}

    # Build QA prompt from template
    # deliverable_files ê²½ë¡œë¡œ ì‹¤ì œ íŒŒì¼ preview ìƒì„±
    file_preview_text = ""
    if deliverable_files:
        try:
            abs_paths = []
            for fp in deliverable_files:
                abs_path = UPLOAD_DIR / fp
                if abs_path.exists():
                    abs_paths.append(str(abs_path))
            if abs_paths:
                preview = generate_all_previews(abs_paths)
                if preview:
                    # íŒŒì¼ previewë¥¼ 3000ìë¡œ ì œí•œ (QA ì»¨í…ìŠ¤íŠ¸ ê³¼ë‹¤ ë°©ì§€)
                    if len(preview) > 3000:
                        preview = preview[:3000] + "\n... (truncated)"
                    file_preview_text = (
                        "\n\n## Actual File Content Preview\n"
                        "(Generated from the real files on disk)\n"
                        f"{preview}"
                    )
        except Exception as e:
            print(f"  âš ï¸  File preview for QA failed: {e}")

    qa_prompt = qa_prompt_template.format(
        instruction=task_info.get("instruction", "")[:3000],
        deliverable_text=(deliverable_text or "")[:2000],
        deliverable_files=json.dumps(deliverable_files),
    )
    # Append file preview after the template-formatted prompt
    qa_prompt += file_preview_text

    model_cfg = condition["model"]
    qa_model = qa_cfg.get("model") or model_cfg["deployment"]
    min_score = qa_cfg.get("min_score", 6)

    qa_messages = [
        {"role": "system", "content": (
            "You are a strict QA inspector for professional deliverables.\n"
            "You MUST respond with ONLY a valid JSON object.\n"
            "No markdown, no code fences, no explanation before or after.\n"
            "No <think> tags. No reasoning. ONLY the JSON.\n"
            "Do NOT wrap your response in ```json``` blocks.\n"
            "\n"
            "IMPORTANT: Keep your response SHORT to avoid truncation.\n"
            "Each issue should be ONE brief sentence (max 15 words).\n"
            "Maximum 3 issues. Suggestion should be ONE sentence.\n"
            "\n"
            "Required format (exactly this structure):\n"
            '{"passed": true, "score": 8, "issues": [], "suggestion": ""}\n'
            "\n"
            "score: integer 1-10\n"
            "issues: list of max 3 short strings\n"
            "suggestion: one short string"
        )},
        {"role": "user", "content": qa_prompt},
    ]

    # QA ì‘ë‹µì€ ì‘ì€ JSONì´ì§€ë§Œ ì˜ë¦¼ ë°©ì§€ë¥¼ ìœ„í•´ ì¶©ë¶„í•œ ì—¬ìœ 
    QA_MAX_TOKENS = 4096

    try:
        # temperature=0 ì§€ì› ì—¬ë¶€ë¥¼ ìºì‹œë¡œ íŒë‹¨ (ë§¤ë²ˆ exception ë°©ì§€)
        if qa_model in _MODELS_NO_TEMPERATURE:
            response, _ = complete(client, qa_model, qa_messages,
                                   max_completion_tokens=QA_MAX_TOKENS)
        else:
            try:
                response, _ = complete(client, qa_model, qa_messages,
                                       temperature=0,
                                       max_completion_tokens=QA_MAX_TOKENS)
            except Exception as temp_err:
                if "temperature" in str(temp_err).lower():
                    _MODELS_NO_TEMPERATURE.add(qa_model)
                    print(f"  â„¹ï¸  {qa_model} doesn't support temperature=0 (cached for session)")
                    response, _ = complete(client, qa_model, qa_messages,
                                           max_completion_tokens=QA_MAX_TOKENS)
                else:
                    raise

        # finish_reason ì²´í¬ â€” ì‘ë‹µ ì˜ë¦¼ ê°ì§€
        finish_reason = getattr(response.choices[0], "finish_reason", None)
        if finish_reason == "length":
            print(f"  âš ï¸  QA response truncated (finish_reason=length)")

        raw = (response.choices[0].message.content or "").strip()
        if not raw:
            print(f"  âš ï¸  QA returned empty response")
            return {
                "passed": None, "score": None,
                "issues": ["QA returned empty response"],
                "suggestion": "", "undetermined": True,
            }

        try:
            result = _extract_json_from_response(raw)
        except (json.JSONDecodeError, ValueError) as parse_err:
            print(f"  âš ï¸  QA JSON parse failed: {parse_err}")
            print(f"     Raw response ({len(raw)} chars): {repr(raw[:300])}")
            return {
                "passed": None, "score": None,
                "issues": [f"QA parse error: {str(parse_err)}"],
                "suggestion": "", "undetermined": True,
                "raw_response": raw[:500],
            }

        score = result.get("score", 10)
        llm_passed = result.get("passed", True)
        passed = score >= min_score

        return {
            "passed": passed,
            "score": score,
            "llm_passed": llm_passed,
            "issues": result.get("issues", []),
            "suggestion": result.get("suggestion", ""),
            "undetermined": False,
        }

    except Exception as e:
        print(f"  âš ï¸  QA API call failed: {e}")
        return {
            "passed": None, "score": None,
            "issues": [f"QA API error: {str(e)}"],
            "suggestion": "", "undetermined": True,
        }


# â”€â”€ File saving (matches main.py _save_files) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _save_files(files: List[dict], task_id: str) -> List[str]:
    """Save generated files to workspace/upload/deliverable_files/<task_id>/."""
    if not files:
        return []

    output_dir = DELIVERABLE_DIR / task_id
    output_dir.mkdir(parents=True, exist_ok=True)

    saved_paths = []
    for file_data in files:
        filename = file_data["filename"]
        content = file_data["content"]
        filepath = output_dir / filename

        if isinstance(content, bytes):
            filepath.write_bytes(content)
        else:
            filepath.write_bytes(content.encode("utf-8"))

        try:
            rel_path = filepath.relative_to(UPLOAD_DIR)
            saved_paths.append(str(rel_path))
        except ValueError:
            saved_paths.append(str(filepath))

    return saved_paths


# â”€â”€ Reflection prompt builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _build_reflection_prompt(
    attempt_num: int,
    qa_score: int,
    qa_issues: list,
    qa_suggestion: str,
    previous_deliverable_text: str,
    min_score: int,
) -> str:
    """Build a structured reflection prompt for QA retry.

    Transforms raw QA feedback into an actionable critique the LLM
    can use to genuinely improve its next attempt.

    Args:
        attempt_num: Current attempt number (1-based, so 2 = first retry)
        qa_score: QA score from previous attempt (0-10)
        qa_issues: List of specific issues identified by QA
        qa_suggestion: Improvement suggestion from QA
        previous_deliverable_text: Summary of what the previous attempt produced
                                   (first 500 chars of deliverable_text)
        min_score: Minimum passing score threshold

    Returns:
        Structured reflection context string to prepend to the retry instruction
    """
    issues_formatted = "\n".join(
        f"  {i+1}. {issue}" for i, issue in enumerate(qa_issues)
    ) if qa_issues else "  (No specific issues recorded)"

    prev_summary = (previous_deliverable_text or "")[:500].strip()
    if len(previous_deliverable_text or "") > 500:
        prev_summary += "... (truncated)"

    return (
        f"[REFLECTION â€” Attempt {attempt_num} | Previous score: {qa_score}/10 "
        f"(target: {min_score}/10)]\n"
        f"\n"
        f"Your previous attempt was reviewed by a QA inspector. "
        f"Here is the structured critique:\n"
        f"\n"
        f"## What you produced (previous attempt)\n"
        f"{prev_summary}\n"
        f"\n"
        f"## Issues identified\n"
        f"{issues_formatted}\n"
        f"\n"
        f"## Improvement suggestion\n"
        f"  {qa_suggestion or 'Address the issues listed above.'}\n"
        f"\n"
        f"## Your task for this attempt\n"
        f"Carefully review each issue above and produce an improved version "
        f"that directly addresses all identified weaknesses. "
        f"Do not simply regenerate the same output â€” make targeted, specific improvements.\n"
        f"\n"
        f"{'='*60}\n"
        f"ORIGINAL TASK:\n"
    )


# â”€â”€ Single task execution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _execute_single_task(
    task_info: dict,
    condition: dict,
    executor,
    execution_mode: str,
    client,
    model: str,
    manifest: Optional[NeedsFilesManifest] = None,
    error_context: Optional[str] = None,
    verbose: bool = False,
) -> dict:
    """Execute a single task and return result dict."""
    task_id = task_info["task_id"]

    # Build prompt
    instruction = task_info["instruction"]
    prompt_cfg = condition["prompt"]
    # system_prompt: only used directly by legacy mode.
    # For code_interpreter/subprocess, codegen YAML's occupation persona takes priority
    # (experiment_prompt["system"] is ignored by render_prompt when codegen YAML has system_message).
    system_prompt = prompt_cfg.get("system", "You are a helpful assistant.")

    experiment_prompt = {
        "system": system_prompt,  # ignored by render_prompt when codegen YAML has system_message
        "prefix": prompt_cfg.get("prefix"),
        "body": prompt_cfg.get("body"),
        "suffix": prompt_cfg.get("suffix"),
    }

    if execution_mode in ("legacy",):
        # Legacy mode doesn't use render_prompt(), so assemble instruction here
        if prompt_cfg.get("prefix"):
            instruction = prompt_cfg["prefix"] + "\n" + instruction
        if prompt_cfg.get("body"):
            instruction = instruction + "\n" + prompt_cfg["body"]
        if prompt_cfg.get("suffix"):
            instruction = instruction + "\n" + prompt_cfg["suffix"]

    # Inject error context for retry
    if error_context:
        if "no deliverable files" in error_context.lower():
            # Keep existing no-files feedback unchanged
            instruction += (
                "\n\n[RETRY - PREVIOUS ATTEMPT FAILED]\n"
                "Your previous attempt did NOT produce any downloadable files.\n"
                "The task requires actual file deliverables, not just a text description.\n"
                "You MUST execute Python code to create and save the required file(s)\n"
                "(e.g., use python-docx for .docx, openpyxl for .xlsx, "
                "reportlab for .pdf, python-pptx for .pptx).\n"
                "Do NOT just describe the deliverable â€” actually generate the file."
            )
        elif error_context.startswith("[REFLECTION"):
            # Reflection retry: structured critique â€” prepend before instruction
            # (the reflection prompt already ends with "ORIGINAL TASK:\n")
            instruction = error_context + instruction
        else:
            # Infrastructure error retry: append error details after instruction
            instruction += (
                "\n\n[RETRY - PREVIOUS ATTEMPT FAILED]\n"
                "The previous code generation produced the following error:\n"
                "---\n"
                f"{error_context}\n"
                "---\n"
                "Please analyze the error above and generate corrected code "
                "that avoids this issue."
            )

    # Resolve reference file paths to absolute + validate existence
    abs_ref_files = None
    ref_files = task_info.get("reference_files", [])
    if ref_files:
        abs_ref_files = []
        for ref_path in ref_files:
            abs_path = DEFAULT_LOCAL_PATH / ref_path
            if abs_path.exists():
                abs_ref_files.append(str(abs_path))
            else:
                print(f"      âš ï¸  Reference file not found: {abs_path}")
        if not abs_ref_files:
            abs_ref_files = None  # all missing â†’ treat as no files

    try:
        start = time.time()

        if execution_mode == "legacy":
            response, latency_ms = complete(
                client,
                model,
                [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": instruction},
                ],
            )
            content = response.choices[0].message.content
            return {
                "task_id": task_id,
                "status": "success",
                "content": content,
                "deliverable_text": content,
                "deliverable_files": [],
                "model": response.model,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens,
                },
                "latency_ms": round(latency_ms, 2),
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

        # Executor mode (code_interpreter / subprocess / json_renderer)
        result = executor.execute(
            task_prompt=instruction,
            model=model,
            reference_files=abs_ref_files,
            occupation=task_info.get("occupation", "professional"),
            experiment_prompt=experiment_prompt,
            verbose=verbose,
        )
        latency_ms = (time.time() - start) * 1000

        if result["success"]:
            deliverable_text = (
                result.get("deliverable_text", "") or result.get("text", "")
            )
            deliverable_files = _save_files(
                result.get("files", []), task_id
            )

            # needs_files gate
            needs_files = task_info.get("needs_files", False)
            if manifest:
                needs_files = manifest.needs_files(task_id)

            if needs_files and not deliverable_files:
                return {
                    "task_id": task_id,
                    "status": "error",
                    "error": "needs_files=True but no deliverable files produced",
                    "content": result.get("text"),
                    "deliverable_text": deliverable_text,
                    "deliverable_files": [],
                    "model": model,
                    "usage": None,
                    "latency_ms": round(latency_ms, 2),
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                }

            return {
                "task_id": task_id,
                "status": "success",
                "content": result["text"],
                "deliverable_text": deliverable_text,
                "deliverable_files": deliverable_files,
                "model": model,
                "usage": None,
                "latency_ms": round(latency_ms, 2),
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }
        else:
            return {
                "task_id": task_id,
                "status": "error",
                "error": result.get("error", "Unknown error"),
                "content": result.get("text"),
                "deliverable_text": result.get("deliverable_text"),
                "deliverable_files": [],
                "model": model,
                "usage": None,
                "latency_ms": round(latency_ms, 2),
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

    except Exception as e:
        return {
            "task_id": task_id,
            "status": "error",
            "error": str(e),
            "content": None,
            "deliverable_text": None,
            "deliverable_files": [],
            "model": model,
            "usage": None,
            "latency_ms": None,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }


# â”€â”€ Incremental save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _save_progress(
    experiment_id: str,
    condition_name: str,
    execution_mode: str,
    total_tasks: int,
    results: List[dict],
    started_at: str,
    path: Path,
) -> None:
    """Atomic incremental save."""
    success = sum(1 for r in results if r.get("status") == "success")
    error = sum(1 for r in results if r.get("status") == "error")

    data = {
        "experiment_id": experiment_id,
        "condition": condition_name,
        "execution_mode": execution_mode,
        "started_at": started_at,
        "summary": {
            "total": total_tasks,
            "completed": len(results),
            "success": success,
            "error": error,
        },
        "results": results,
    }

    tmp_path = path.with_suffix(".json.tmp")
    with open(tmp_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    tmp_path.rename(path)


# â”€â”€ Progress helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _get_failed_task_ids(progress: dict) -> list:
    """progress.jsonì—ì„œ retriable status íƒœìŠ¤í¬ ì¶”ì¶œ."""
    failed = []
    for r in progress.get("results", []):
        if r.get("status") in RETRIABLE_STATUSES:
            failed.append({
                "task_id": r["task_id"],
                "status": r["status"],
                "error": r.get("error", ""),
            })
    return failed


def _update_progress_result(progress: dict, new_result: dict) -> dict:
    """progress.json resultsì—ì„œ task_id ì¼ì¹˜í•˜ëŠ” ì˜¤ë¸Œì íŠ¸ë¥¼ êµì²´."""
    updated = []
    replaced = False
    for r in progress.get("results", []):
        if r["task_id"] == new_result["task_id"]:
            updated.append(new_result)
            replaced = True
        else:
            updated.append(r)
    if not replaced:
        updated.append(new_result)
    progress["results"] = updated
    return progress


# â”€â”€ Main inference loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def run_inference(
    execution_mode: str = None,
    max_retries: int = None,
    resume: bool = True,
    condition_key: str = "condition_a",
    resume_max_rounds: int = None,
    verbose: bool = False,
):
    """Run inference for all prepared tasks with multi-round resume.

    Resume rounds automatically re-execute failed tasks from progress.json,
    replacing the error objects in-place on success.
    """

    # 1. Load prepared tasks
    prepared_path = WORKSPACE_DIR / "step1_tasks_prepared.json"
    if not prepared_path.exists():
        print(f"âŒ {prepared_path} not found. Run step1_prepare_tasks.sh first.")
        sys.exit(1)

    with open(prepared_path, "r", encoding="utf-8") as f:
        prepared = json.load(f)

    tasks = prepared["tasks"]
    condition = prepared[condition_key]
    if condition is None:
        print(f"âŒ {condition_key} not found in config")
        sys.exit(1)

    experiment_id = prepared["experiment_id"]
    model = condition["model"]["deployment"]
    condition_name = condition["name"]

    # Resolve settings: CLI override > YAML execution block > defaults
    execution_cfg = prepared.get("execution", {})
    if execution_mode is None:
        execution_mode = execution_cfg.get("mode", prepared.get("execution_mode", "subprocess"))
    if max_retries is None:
        max_retries = execution_cfg.get("max_retries", prepared.get("max_retries", 3))
    if resume_max_rounds is None:
        resume_max_rounds = execution_cfg.get("resume_max_rounds", 3)

    print(f"\n{'='*60}")
    print(f"ğŸš€ Step 2: Run Inference")
    print(f"{'='*60}")
    print(f"   Experiment:         {experiment_id}")
    print(f"   Condition:          {condition_name}")
    print(f"   Model:              {model}")
    print(f"   Mode:               {execution_mode}")
    print(f"   Tasks:              {len(tasks)}")
    print(f"   Max retries:        {max_retries} (per task, infra)")
    print(f"   Resume max rounds:  {resume_max_rounds} (re-run failed tasks)")

    # QA config
    qa_cfg = condition.get("qa", {})
    qa_enabled = qa_cfg.get("enabled", False)
    qa_max_retries = qa_cfg.get("max_retries", 2) if qa_enabled else 0
    if qa_enabled:
        qa_model = qa_cfg.get("model") or model
        print(f"   Self-QA:            enabled (min_score={qa_cfg.get('min_score', 6)}, "
              f"max_retries={qa_max_retries}, model={qa_model})")

    # 2. Create LLM client (provider-aware)
    provider = condition.get("model", {}).get("provider", "azure")

    if provider in ("azure", "azure_openai"):
        endpoint = os.getenv("AZURE_OPENAI_ENDPOINT") or os.getenv("AZURE_ENDPOINT")
        api_key = os.getenv("AZURE_OPENAI_API_KEY") or os.getenv("AZURE_API_KEY")
        if not endpoint or not api_key:
            print("âŒ Missing Azure credentials. Set AZURE_OPENAI_ENDPOINT + AZURE_OPENAI_API_KEY")
            sys.exit(1)
        client = create_provider_client("azure", endpoint=endpoint, api_key=api_key)
        print(f"   Client:             Azure @ {endpoint}")

    elif provider == "openai":
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âŒ Missing OpenAI credentials. Set OPENAI_API_KEY")
            sys.exit(1)
        client = create_provider_client("openai", api_key=api_key)
        print(f"   Client:             OpenAI (native)")

    elif provider == "anthropic":
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            print("âŒ Missing Anthropic credentials. Set ANTHROPIC_API_KEY")
            sys.exit(1)
        client = create_provider_client("anthropic", api_key=api_key)
        print(f"   Client:             Anthropic")

    else:
        print(f"âŒ Unsupported provider: '{provider}'. Use: azure, openai, anthropic")
        sys.exit(1)

    # 3. Initialize executor (no silent fallback â€” fail loudly)
    try:
        executor = TaskExecutor(mode=execution_mode, llm_client=client)
    except Exception as e:
        print(f"âŒ Executor init failed for mode '{execution_mode}': {e}")
        print(f"   Fix the issue or change execution.mode in your YAML config.")
        sys.exit(1)

    # 4. Load manifest
    manifest = None
    try:
        manifest = NeedsFilesManifest.load()
        print(f"   Manifest:           {manifest}")
    except FileNotFoundError:
        print("   âš ï¸  Manifest not found â€” skipping file checks")

    # 5. Build task lookup
    task_map = {t["task_id"]: t for t in tasks}
    total = len(tasks)
    progress_path = WORKSPACE_DIR / "step2_inference_progress.json"
    started_at = datetime.now(timezone.utc).isoformat()

    # â”€â”€ Helper: execute one task with QA loop â”€â”€

    def _run_task_with_qa(task: dict, error_context: str = None) -> dict:
        """Execute one task with infra retries + Self-QA retry loop.

        QA status handling:
        - passed=True  â†’ success (QA í†µê³¼)
        - passed=False â†’ success (íŒŒì¼ ìƒì„± = LLM ì„±ëŠ¥, QA ì ìˆ˜ë§Œ ë‚®ìŒ)
        - undetermined â†’ ì¬ì‹œë„ í›„ ë§ˆì§€ë§‰ì´ë©´ success (score=None)

        Best-swap: QA ì¬ì‹œë„ ì‹œ ì´ì „ best íŒŒì¼ì„ ë°±ì—….
        ìƒˆ ê²°ê³¼ê°€ ë” ì¢‹ìœ¼ë©´ ë°±ì—… ì‚­ì œ, ë” ë‚˜ì˜ë©´ ë°±ì—…ì—ì„œ ë³µì›.
        """
        import shutil
        import tempfile

        task_id = task["task_id"]
        qa_attempts = 0
        last_qa_feedback = error_context
        reflection_history = []

        # â”€â”€ best-swap state â”€â”€
        best_result = None
        best_score = -1
        best_qa = None
        backup_dir = None  # best íŒŒì¼ ë°±ì—… ê²½ë¡œ

        def _backup_best_files():
            """í˜„ì¬ uploadì˜ deliverable_filesë¥¼ ë°±ì—…."""
            nonlocal backup_dir
            task_dir = DELIVERABLE_DIR / task_id
            if task_dir.exists() and any(task_dir.iterdir()):
                backup_dir = tempfile.mkdtemp(prefix=f"qa_best_{task_id}_")
                shutil.copytree(task_dir, Path(backup_dir) / "files", dirs_exist_ok=True)

        def _restore_best_files():
            """ë°±ì—…ì—ì„œ best íŒŒì¼ì„ uploadë¡œ ë³µì›."""
            nonlocal backup_dir
            if backup_dir:
                task_dir = DELIVERABLE_DIR / task_id
                if task_dir.exists():
                    shutil.rmtree(task_dir, ignore_errors=True)
                shutil.copytree(Path(backup_dir) / "files", task_dir)

        def _cleanup_backup():
            """ë°±ì—… ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ."""
            nonlocal backup_dir
            if backup_dir:
                shutil.rmtree(backup_dir, ignore_errors=True)
                backup_dir = None

        try:
            while True:
                if qa_attempts > 0:
                    print(f"\n      ğŸ”„ Re-executing task "
                          f"(QA attempt {qa_attempts + 1}/{qa_max_retries + 1})...",
                          end=" ", flush=True)

                    # ì¬ì‹¤í–‰ ì „: í˜„ì¬ best íŒŒì¼ ë°±ì—… í›„ task_dir ë¹„ìš°ê¸°
                    _cleanup_backup()  # ì´ì „ ë°±ì—… ì •ë¦¬
                    _backup_best_files()

                    # task_dir ë¹„ìš°ê¸° â€” ì´ì „ íŒŒì¼ì´ ë‚¨ì•„ ìˆìœ¼ë©´ LLMì´ ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ
                    # íŒŒì¼ì„ ìƒì„±í–ˆì„ ë•Œ êµ¬/ì‹  íŒŒì¼ì´ ê³µì¡´í•˜ê²Œ ë¨
                    task_dir = DELIVERABLE_DIR / task_id
                    if task_dir.exists():
                        shutil.rmtree(task_dir, ignore_errors=True)

                result = _execute_single_task(
                    task, condition, executor, execution_mode,
                    client, model, manifest,
                    error_context=last_qa_feedback,
                    verbose=verbose,
                )

                # If execution failed, return best if available
                if result["status"] != "success":
                    if best_result is not None:
                        _restore_best_files()
                        _cleanup_backup()
                        print(f"\n      âš ï¸  Re-execution failed, "
                              f"keeping best result (score={best_score})",
                              end=" ", flush=True)
                        return best_result
                    break

                if not qa_enabled:
                    break

                # Run Self-QA (íŒŒì¼ì´ workspaceì— ì €ì¥ëœ ìƒíƒœ â†’ file_previewë¡œ ì‹¤ì œ ë‚´ìš© í™•ì¸)
                qa_result_info = _run_self_qa(
                    task, condition,
                    result.get("deliverable_text", ""),
                    result.get("deliverable_files", []),
                    client,
                )
                result["qa"] = qa_result_info

                # Record this QA attempt in history
                reflection_history.append({
                    "attempt": qa_attempts + 1,          # 1-based
                    "score": qa_result_info.get("score"),
                    "passed": qa_result_info.get("passed"),
                    "undetermined": qa_result_info.get("undetermined", False),
                })

                # â”€â”€ best-swap: ì ìˆ˜ ë¹„êµ â”€â”€
                current_score = qa_result_info.get("score")
                if current_score is None:
                    current_score = -1  # undetermined

                if current_score > best_score:
                    # ìƒˆ ê²°ê³¼ê°€ ë” ì¢‹ìŒ â†’ ë°±ì—… ì‚­ì œ, ìƒˆ ê²°ê³¼ë¥¼ bestë¡œ
                    _cleanup_backup()
                    best_score = current_score
                    best_qa = qa_result_info
                    best_result = result
                    if qa_attempts > 0:
                        print(f"\n      ğŸ“ˆ New best score: {best_score}",
                              end=" ", flush=True)
                else:
                    # ìƒˆ ê²°ê³¼ê°€ ë” ë‚˜ì¨ â†’ ë°±ì—…ì—ì„œ best íŒŒì¼ ë³µì›
                    if backup_dir:
                        _restore_best_files()
                        _cleanup_backup()
                    print(f"\n      ğŸ“‰ Score {current_score} â‰¤ best {best_score}, "
                          f"keeping previous best",
                          end=" ", flush=True)
                    # best_resultì˜ deliverable_files ê²½ë¡œëŠ” ê·¸ëŒ€ë¡œ ìœ íš¨ (restoreë¨)

                # â”€â”€ Handle undetermined â”€â”€
                if qa_result_info.get("undetermined"):
                    qa_attempts += 1
                    if qa_attempts >= qa_max_retries:
                        if best_qa:
                            best_qa["passed"] = True
                        if best_result:
                            best_result["qa"] = best_qa
                        print(f"\n      âš ï¸  QA undetermined on final attempt â€” "
                              f"saving as success (undetermined)",
                              end=" ", flush=True)
                        break
                    print(f"\n      âš ï¸  QA undetermined, "
                          f"retrying ({qa_attempts}/{qa_max_retries})...",
                          end=" ", flush=True)
                    last_qa_feedback = None
                    continue

                # â”€â”€ QA í†µê³¼ â”€â”€
                if qa_result_info["passed"]:
                    break

                # â”€â”€ QA ì‹¤íŒ¨ (score < min_score) â”€â”€
                qa_attempts += 1
                if qa_attempts >= qa_max_retries:
                    print(f"\n      âš ï¸  QA max retries reached "
                          f"(best score={best_score}) â€” "
                          f"saving as success",
                          end=" ", flush=True)
                    break

                # Build structured reflection prompt for retry
                last_qa_feedback = _build_reflection_prompt(
                    attempt_num=qa_attempts + 1,
                    qa_score=qa_result_info["score"],
                    qa_issues=qa_result_info.get("issues", []),
                    qa_suggestion=qa_result_info.get("suggestion", ""),
                    previous_deliverable_text=result.get("deliverable_text", ""),
                    min_score=qa_cfg.get("min_score", 6),
                )
                print(f"\n      ğŸ” QA: score={qa_result_info['score']}, "
                      f"retrying ({qa_attempts}/{qa_max_retries})...",
                      end=" ", flush=True)

        finally:
            _cleanup_backup()  # í•­ìƒ ë°±ì—… ì •ë¦¬

        if best_result is not None:
            best_result["reflection_history"] = reflection_history
            best_result["reflection_attempts"] = len(reflection_history)
            if len(reflection_history) > 0:
                best_result["reflection_final_score"] = best_score
            return best_result
        result["reflection_history"] = reflection_history
        result["reflection_attempts"] = len(reflection_history)
        if len(reflection_history) > 0 and result.get("status") == "success":
            result["reflection_final_score"] = best_score if best_score >= 0 else None
        return result

    # â”€â”€ Helper: print result status â”€â”€

    def _print_status(result: dict):
        if result["status"] == "success":
            file_count = len(result.get("deliverable_files", []))
            latency = result.get("latency_ms", 0) or 0
            qa_info = ""
            if result.get("qa"):
                qa_info = f", QA={result['qa']['score']}"
            reflection_info = ""
            if result.get("reflection_attempts", 0) > 0:
                reflection_info = f", reflectÃ—{result['reflection_attempts']}"
            print(f"âœ“ ({latency:.0f}ms, {file_count} files{qa_info}{reflection_info})")
        elif result["status"] == "qa_failed":
            qa = result.get("qa", {})
            print(f"âœ— QA failed (score={qa.get('score', '?')})")
        else:
            print(f"âœ— {result.get('error', 'Unknown')}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6. Initial run OR load existing progress
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    progress = None
    if resume and progress_path.exists():
        with open(progress_path, "r", encoding="utf-8") as f:
            progress = json.load(f)

        completed_count = sum(1 for r in progress.get("results", [])
                              if r.get("status") == "success")
        failed_count = sum(1 for r in progress.get("results", [])
                           if r.get("status") in RETRIABLE_STATUSES)
        print(f"\n   â™»ï¸  Loaded progress: {completed_count} succeeded, "
              f"{failed_count} failed, "
              f"round {progress.get('resume_round', 0)}")

    if progress is None:
        # === INITIAL RUN: ëª¨ë“  íƒœìŠ¤í¬ ì‹¤í–‰ ===
        print(f"\nâ”€â”€ Round 0: Initial Run ({total} tasks) â”€â”€")
        progress = {
            "experiment_id": experiment_id,
            "condition": condition_name,
            "execution_mode": execution_mode,
            "total_tasks": total,
            "started_at": started_at,
            "resume_round": 0,
            "results": [],
        }

        for i, task in enumerate(tasks):
            task_id = task["task_id"]
            print(f"   [{i+1}/{total}] {task_id} "
                  f"({task['sector']}/{task['occupation']})...",
                  end=" ", flush=True)

            result = _run_task_with_qa(task)
            progress["results"].append(result)
            _print_status(result)

            # Incremental save
            _save_progress(
                experiment_id, condition_name, execution_mode,
                total, progress["results"], started_at, progress_path,
            )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 7. Resume rounds: progress.jsonì˜ error íƒœìŠ¤í¬ë¥¼ ìë™ ì¬ì‹¤í–‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    for round_num in range(1, resume_max_rounds + 1):
        failed = _get_failed_task_ids(progress)

        if not failed:
            print(f"\nâœ… No failed tasks â€” skipping resume rounds")
            break

        print(f"\nâ”€â”€ Resume Round {round_num}/{resume_max_rounds}: "
              f"{len(failed)} failed tasks â”€â”€")

        recovered = 0
        for fi, fail_info in enumerate(failed, 1):
            task_id = fail_info["task_id"]
            task = task_map.get(task_id)
            if task is None:
                print(f"   âš ï¸ {task_id} not in task_map, skipping")
                continue

            print(f"   [{fi}/{len(failed)}] ğŸ”„ {task_id} "
                  f"(prev: {fail_info['status']})...",
                  end=" ", flush=True)

            result = _run_task_with_qa(task, error_context=fail_info.get("error"))
            result["resume_round"] = round_num

            # progress.jsonì—ì„œ í•´ë‹¹ task_id ì˜¤ë¸Œì íŠ¸ë¥¼ ì§ì ‘ êµì²´
            progress = _update_progress_result(progress, result)
            progress["resume_round"] = round_num
            _print_status(result)

            # Incremental save
            _save_progress(
                experiment_id, condition_name, execution_mode,
                total, progress["results"], started_at, progress_path,
            )

            if result["status"] == "success":
                recovered += 1

        still_failed = _get_failed_task_ids(progress)
        print(f"\n   Round {round_num} summary: "
              f"{recovered}/{len(failed)} recovered, "
              f"{len(still_failed)} still failing")

        if not still_failed:
            print(f"   ğŸ‰ All tasks recovered!")
            break

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 8. Final summary & save
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    results = progress.get("results", [])
    success = sum(1 for r in results if r["status"] == "success")
    errors = sum(1 for r in results if r["status"] == "error")
    qa_failed = sum(1 for r in results if r.get("status") == "qa_failed")

    final_output = {
        "experiment_id": experiment_id,
        "experiment_name": prepared.get("experiment_name", ""),
        "source": prepared.get("source", ""),
        "condition": condition_name,
        "execution_mode": execution_mode,
        "model": model,
        "started_at": started_at,
        "completed_at": datetime.now(timezone.utc).isoformat(),
        "resume_rounds_used": progress.get("resume_round", 0),
        "summary": {
            "total": len(results),
            "success": success,
            "error": errors,
            "qa_failed": qa_failed,
        },
        "results": results,
    }

    output_path = WORKSPACE_DIR / "step2_inference_results.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=2, ensure_ascii=False, default=str)

    print(f"\n{'='*60}")
    print(f"âœ… Step 2 complete: {output_path}")
    print(f"   Success:            {success}/{len(results)}")
    print(f"   Error:              {errors}/{len(results)}")
    if qa_failed:
        print(f"   QA failed:          {qa_failed}/{len(results)}")
    print(f"   Resume rounds used: {progress.get('resume_round', 0)}/{resume_max_rounds}")
    print(f"{'='*60}")


def main():
    parser = argparse.ArgumentParser(description="Step 2: Run inference")
    parser.add_argument(
        "--mode",
        default=None,
        choices=["code_interpreter", "subprocess", "json_renderer"],
        help="Execution mode (overrides YAML execution.mode)",
    )
    parser.add_argument(
        "--max-retries", type=int, default=None,
        help="Infra retries per task (overrides YAML execution.max_retries)",
    )
    parser.add_argument(
        "--resume-max-rounds", type=int, default=None,
        help="Max resume rounds for failed tasks (overrides YAML execution.resume_max_rounds)",
    )
    parser.add_argument(
        "--no-resume", action="store_true",
        help="Start fresh (ignore previous progress)",
    )
    parser.add_argument(
        "--condition",
        default="condition_a",
        choices=["condition_a", "condition_b"],
        help="Which condition to run",
    )
    parser.add_argument(
        "--verbose", action="store_true",
        help="Print detailed debug info about API response structure (code_interpreter mode)",
    )
    args = parser.parse_args()

    run_inference(
        execution_mode=args.mode,
        max_retries=args.max_retries,
        resume=not args.no_resume,
        condition_key=args.condition,
        resume_max_rounds=args.resume_max_rounds,
        verbose=args.verbose,
    )


if __name__ == "__main__":
    main()
