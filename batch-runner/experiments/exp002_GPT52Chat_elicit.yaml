# Experiment Configuration
# Full run with GPT-5.2 Chat on all 220 tasks + PROMPT_ELICIT_CAPABILITIES_SUFFIX From A.3 in https://arxiv.org/pdf/2510.04374
# Compare with exp001_GPT52Chat_baseline (default suffix)
# If condition_b is absent, automatically runs as Single Test 

experiment:
  id: "exp002_GPT52Chat_elicit"
  name: "GPT-5.2 Chat Elicit Capabilities (Full 220 tasks)"
  description: "Full run with GPT-5.2 Chat on all 220 tasks using PROMPT_ELICIT_CAPABILITIES_SUFFIX (PNG formatting checks, CONFIDENCE score). Compare with exp001 baseline."
  author: "Hyeonsang Jeon"
  created_at: "2026-02-25"

# Variable control
control:
  fixed:
    - model
    - tasks
    - temperature
  changed:
    - prompt_strategy

# Data filter
data:
  source: "HyeonSang/exp002_GPT52Chat_elicit"
  filter:
    sector: null
    occupation: null
    sample_size: null

# Condition A: Elicit (no condition_b → Single Test)
condition_a:
  name: "Elicit"
  model:
    provider: "azure" #"anthropic", "azure", "openai", "custom"
    deployment: "gpt-5.2-chat"
    temperature: 0.0
    seed: 42
  prompt:
    system: |
      You are a helpful assistant that completes professional tasks.
      When the task requires creating files (e.g., Excel, Word, PDF, PowerPoint),
      write Python code that generates those files and save them to the current directory.
      Use libraries such as openpyxl, python-docx, reportlab, python-pptx, and Pillow.
      Always produce complete, runnable code.
    prefix: null
    body: null
    suffix: |
      If the task requires creating deliverable files (documents, spreadsheets,
      presentations, reports, etc.), you MUST use Python code to generate the
      actual file(s) and save them to the current directory.
      Do NOT just describe what you would create — actually execute code to produce
      downloadable files. The task is NOT complete unless the files are saved.

      In your text response, do NOT include any file download links or sandbox URLs
      (e.g. "[Download ...](sandbox:/mnt/data/...)" or similar).
      Only provide a plain-text summary of what was produced.

      Special characters
      - Never use the character ‑ (U+2011), since it will render poorly on some people's computers. Instead, always use - (U+002D) instead.
      - Avoid emojis, nonstandard bullet points, and other special characters unless there is an extremely good reason to use them, since these render poorly on some people's computers.

      Graphics embedded within PDFs/slides
      - Make sure that any diagrams or plots are large enough to be legible (though not so large that they are ugly or cut off). In most cases they should be at least half the page width.
      - Plots and charts to visualize data are good. Simple graphics (like a flowchart with arrows) are good. But complicated visuals constructed by overlaying shapes into an image often appear unprofessional.

      PDFs
      - Always use LibreOffice to create the PDF (it must be LibreOffice! If LibreOffice is not installed, you can install it yourself). Other libraries sometimes show weird artifacts on some computers.

      Fonts
      - Always use fonts which are available across all platforms. We recommend Noto Sans / Noto Serif unless there is an extremely good reason to use something else.
      - If you must use another font, embed the font in the pptx/word/etc doc.

      Deliverable text
      - Do not link to submitted files in the deliverable text (links are not supported on the interface where these will be viewed).
      - Ideal deliverable text is concise and to the point, without any unnecessary fluff. 4 sentences max.
      - Any deliverables the user asked for should be in files in the container, NOT purely in the deliverable text.
      - If a portion of the task was unsolvable (for instance, because internet was not available), mention this in the deliverable text.
      - Your submission should be complete and self-contained. Even if you are unable to fully complete the task due to limitations in the environment, produce as close to a complete solution as possible.

      Verbosity
      - Always be clear and comprehensive, but avoid extra verbosity when possible.

      Filetypes
      - If the prompt does not request a specific filetype, use standard filetypes like PDF, PPTX, DOCX, XLSX, MP4, ZIP, etc.

      Video files (mp4, mov)
      - Extract a string of images from the video files and check the images to see whether the visual elements are corrupted.

      Mandatory formatting checks
      - Before you submit your deliverable, you MUST perform the following mandatory formatting checks. Take your time, do these thoroughly, they are extremely important!

      STEP 1: Convert all visual deliverables to PNGs using LibreOffice. This includes pptx, docx, pdf, xlsx, etc. Convert it so that each page or slide is a separate PNG. This is mandatory; you will fail the task if you skip this step (unless there are no visual deliverables). You still need to submit the original deliverables in the original format to the user, this is purely for checking formatting.

      STEP 2: Display the PNGs. You are trying to see if the text or graphics are cut off, overlapping, distorted, blank, hard to read (dark text on dark background or light text on light background), or otherwise poorly formatted. Look at each image thoroughly, zoom in if you need to see more closely. Remember that the image you see is an entire slide, so if any text or graphic is cut off, this is an error with the deliverable.

      STEP 3: Programmatic formatting checks. For highly visual submissions (e.g. pptx, pdf), write programmatic checks to make sure there are no blank pages, text/graphics cut off the page, or overlapping text or graphics (except intentional ones). Also check that if there is a page or slide limit, it is respected.

      STEP 4: Summarize the prompt's deliverable instructions, and match that to the portion of the deliverable that addresses it.

      STEP 5: Right before submitting, check that the deliverables you have produced are exactly what you want to submit: deliverables should contain exactly the files you want to submit, with no extra files. Check that these deliverables are not corrupted in any way by opening each to make sure it is well-formatted.

      If any of these checks reveal a formatting issue, fix them and go through steps 1-5 again. Take your time, be thorough, remember you can zoom in on details.

      This is IMPORTANT and MANDATORY, go through each step one-by-one meticulously! Every formatting error is a MAJOR ISSUE THAT YOU NEED TO FIX! There is no time limit, be thorough, go slide by slide or page by page.

      Finally - on the last line of your output text, add CONFIDENCE[XX], where XX is an integer between 0 and 100, inclusive, indicating your confidence that the submission is correct, follows instructions, and is well-formatted.

      Before writing any code, ALWAYS inspect the actual structure of any
      reference files first (df.columns, df.dtypes, df.head()).
      Never hardcode column names or assume data types.

  # Self-QA: LLM inspects its own output
  qa:
    enabled: true
    max_retries: 1              # max regeneration attempts on QA failure
    model: null                 # null = same model as generation
    min_score: 5                # scores below this fail QA (1-10)
    prompt: |
      You are a strict QA inspector for professional deliverables.

      ## Original Task
      {instruction}

      ## Generated Output
      - Text response: {deliverable_text}
      - Files produced: {deliverable_files}

      ## Inspection Criteria
      1. Does the output address ALL requirements in the original task?
      2. Are all required files produced with correct types and content?
      3. Is the text response complete and professional?
      4. Are there any obvious errors, missing elements, or placeholder content?

      ## Response Format (JSON only)
      ```json
      {{
        "passed": true,
        "score": 8,
        "issues": [],
        "suggestion": ""
      }}
      ```

# Execution settings
execution:
  #mode: "subprocess"          # code_interpreter | subprocess | json_renderer
  mode: "code_interpreter"     # code_interpreter | subprocess | json_renderer
  max_retries: 5               # infra retries per task (API errors, timeouts, Python execution errors)
  resume_max_rounds: 1         # auto re-run rounds for error tasks in progress.json (0 = no resume)