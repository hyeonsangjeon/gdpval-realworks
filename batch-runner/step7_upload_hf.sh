#!/usr/bin/env bash
# Step 7: Upload dataset to HuggingFace Hub (openai/gdpval êµ¬ì¡°ì™€ ë™ì¼)
#
# Usage:
#   HF_TOKEN=hf_xxx ./step7_upload_hf.sh            # ì™„ì „ ìë™ (220í–‰ full ê²€ì¦)
#   HF_TOKEN=hf_xxx ./step7_upload_hf.sh [repo_id]  # repo override
#   HF_TOKEN=hf_xxx ./step7_upload_hf.sh --test      # smoke test (parquet ì‹¤ì œ í–‰ ìˆ˜ë¡œ ê²€ì¦)
#
# ì—…ë¡œë“œ ëŒ€ìƒ: README.md, data/train-*.parquet, deliverable_files/**
# ì œì™¸ ëŒ€ìƒ: .cache/, train/, dataset_dict.json ë“± ìºì‹œ ì•„í‹°íŒ©íŠ¸
# reference_files/** ëŠ” duplicate ëœ ë² ì´ìŠ¤ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì‚­ì œ/ì—…ë¡œë“œ ì•ˆ í•¨)

set -euo pipefail
cd "$(dirname "$0")"

UPLOAD_DIR="workspace/upload"
FORCE_TEST=""

# Parse arguments
for arg in "$@"; do
    case "$arg" in
        --test) FORCE_TEST="1" ;;
        *)      REPO_ARG="$arg" ;;
    esac
done

# â”€â”€ expected row count ê²°ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EXPECTED_ROWS=220
TEST_MODE_SOURCE=""

if [ -n "$FORCE_TEST" ]; then
    # --test: parquetì˜ ì‹¤ì œ í–‰ ìˆ˜ë¥¼ ì½ì–´ ê²€ì¦ (smoke testìš©)
    EXPECTED_ROWS=$(python3 -c "
import pandas as pd, glob
parquets = sorted(glob.glob('${UPLOAD_DIR}/data/train-*.parquet'))
print(len(pd.read_parquet(parquets[0])) if parquets else 220)
" 2>/dev/null || echo 220)
    TEST_MODE_SOURCE="--test (rows=${EXPECTED_ROWS})"
fi

# repo_id ê²°ì •: ì¸ì > workspace/step2_inference_results.jsonì˜ "source" > ê¸°ë³¸ê°’
if [ -n "${REPO_ARG:-}" ]; then
  REPO_ID="$REPO_ARG"
  echo "â„¹ï¸  Repo: $REPO_ID  (ì¸ì ì§€ì •)"
elif [ -f "workspace/step2_inference_results.json" ]; then
  REPO_ID=$(python3 -c "
import json, sys
d = json.load(open('workspace/step2_inference_results.json'))
src = d.get('source', '').strip()
if not src:
    print('hyeonsangjeon/gdpval-realwork-results')
else:
    print(src)
")
  echo "â„¹ï¸  Repo: $REPO_ID  (step2_inference_results.json 'source'ì—ì„œ ì½ìŒ)"
else
  REPO_ID="hyeonsangjeon/gdpval-realwork-results"
  echo "â„¹ï¸  Repo: $REPO_ID  (ê¸°ë³¸ê°’)"
fi

if [ -z "${HF_TOKEN:-}" ]; then
  echo "âŒ HF_TOKEN not set."
  echo "   export HF_TOKEN=hf_xxx"
  exit 1
fi

if [ ! -d "$UPLOAD_DIR" ]; then
  echo "âŒ Upload directory not found: $UPLOAD_DIR"
  echo "   Run step2 (inference) and step4 (fill parquet) first."
  exit 1
fi

echo "ğŸ¤— HuggingFace Upload"
echo "   Repo:          $REPO_ID"
echo "   Source:        $UPLOAD_DIR"
if [ -n "$TEST_MODE_SOURCE" ]; then
  echo "   Mode:          Test â€” ${TEST_MODE_SOURCE}"
else
  echo "   Mode:          Full (220 rows)"
fi
echo ""

# â”€â”€ Pre-upload validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "ğŸ” Pre-upload validation..."
export REPO_ID UPLOAD_DIR EXPECTED_ROWS

python3 - <<'VALIDATE_EOF'
import sys, os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) or ".")
from core.repo_bootstrapper import validate_pre_upload
expected = int(os.environ.get("EXPECTED_ROWS", "220"))
errors = validate_pre_upload(local_path=os.environ["UPLOAD_DIR"], expected_rows=expected)
if errors:
    print("âŒ Pre-upload validation FAILED:")
    for e in errors:
        print(f"   â€¢ {e}")
    sys.exit(1)
print(f"âœ“ Pre-upload validation passed  (rows={expected})")
VALIDATE_EOF

# â”€â”€ Upload â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python3 - <<'PYEOF'
import os
from pathlib import Path
from huggingface_hub import HfApi, create_repo

repo_id = os.environ["REPO_ID"]
data_dir = Path(os.environ["UPLOAD_DIR"])
token = os.environ["HF_TOKEN"]

api = HfApi(token=token)

# Create repo if not exists
try:
    create_repo(repo_id, repo_type="dataset", exist_ok=True, token=token)
    print(f"âœ“ Repository ready: {repo_id}")
except Exception as e:
    print(f"âš ï¸  Repo creation: {e}")

# Upload only what matches openai/gdpval structure
# reference_files/** is intentionally EXCLUDED â€” it comes from the
# duplicated openai/gdpval base and should never be re-uploaded.
INCLUDE = [
    "README.md",
    "data/train-*.parquet",
    "deliverable_files/**",
]

IGNORE = [
    ".cache/**",
    "train/**",
    "dataset_dict.json",
    "*.arrow",
    "*.lock",
    "__pycache__/**",
    "state.json",
    "dataset_info.json",
]

# delete_patterns: wipe data/** and deliverable_files/** from remote FIRST
# so stale files from previous runs don't persist.
DELETE = [
    "data/**",
    "deliverable_files/**",
]

print(f"\nğŸ“¤ Uploading files (with remote cleanup)...")
print(f"   Include: {INCLUDE}")
print(f"   Delete (remote): {DELETE}")
print(f"   Ignore:  {IGNORE}")

api.upload_folder(
    folder_path=str(data_dir),
    repo_id=repo_id,
    repo_type="dataset",
    allow_patterns=INCLUDE,
    ignore_patterns=IGNORE,
    delete_patterns=DELETE,
    commit_message="Update dataset with experiment results",
)

print(f"\nâœ… Upload complete!")
print(f"   https://huggingface.co/datasets/{repo_id}/tree/main")
PYEOF
